{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Note: honestly, I was pretty tempted to create a model that preps data based on type of data and model it will feed into,\n# but opted to choose this simplified generalized version due to time constraints","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# here is the summary of agents used and in order:\n# Standardize: Fix types, remove currency symbols, unify text casing.\n# Date: Extract features (Year/Month) from timestamps.\n# Duplicates: Remove exact row matches (Safety: <1%).\n# Grouper: Group rare categories into \"Other\" (Safety: <1%).\n# Nulls: Drop bad cols/rows or Impute (Median/Mode).\n# Correlation: Drop redundant features (Correlation > 95%).\n# Skew: Log-transform positive outliers.\n# Encoding: Convert categories to numbers (One-Hot vs Label).\n# Scaler: Split Train/Test and Standardize (Prevent Leakage).\n# Bias: Fixes Statistical Bias and Class Imbalance on Train set.\n# Mastermind: orchestrates the code of these 10 agents to work in sequential order on a given dataset.","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# setup keys\nimport os\nfrom kaggle_secrets import UserSecretsClient\n\ntry:\n    GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n    os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n    print(\"‚úÖ Setup and authentication complete.\")\nexcept Exception as e:\n    print(\n        f\"üîë Authentication Error: Please make sure you have added 'GOOGLE_API_KEY' to your Kaggle secrets. Details: {e}\"\n    )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import libraries\nfrom google.genai import types\n\nfrom google.adk.agents import LlmAgent\nfrom google.adk.models.google_llm import Gemini\nfrom google.adk.runners import InMemoryRunner\nfrom google.adk.sessions import InMemorySessionService\nfrom google.adk.tools import google_search, AgentTool, ToolContext\nfrom google.adk.code_executors import BuiltInCodeExecutor\n\nprint(\"‚úÖ ADK components imported successfully.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# configure retry options\nretry_config = types.HttpRetryOptions(\n    attempts=5,  # Maximum retry attempts\n    exp_base=7,  # Delay multiplier\n    initial_delay=1,\n    http_status_codes=[429, 500, 503, 504],  # Retry on these HTTP errors\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport random\n# using a randomized dataset, in production replace with an actual dataset\n# Set seed for reproducibility\nnp.random.seed(42)\n\ndef generate_messy_dataset(rows=1000):\n    print(\"‚ö†Ô∏è Generating The Doomed Dataset...\")\n    \n    # 1. BASE DATA & SKEW (Triggers SkewAgent)\n    # Generate a log-normal distribution (Right skewed)\n    transaction_amt = np.random.lognormal(mean=2, sigma=1, size=rows)\n    \n    # 2. REDUNDANT FEATURES (Triggers CorrelationAgent)\n    # Celsius and Fahrenheit are perfectly correlated\n    temp_c = np.random.normal(25, 5, rows)\n    temp_f = temp_c * 9/5 + 32\n    \n    # 3. MESSY STRINGS & CURRENCY (Triggers StandardizeAgent)\n    # Includes whitespace, different cases, and symbols\n    cities = [\"  new york \", \"New York\", \"SF\", \"sf \", \"chicago\", \"Chicago\", \"  Austin\"]\n    city_col = np.random.choice(cities, rows)\n    \n    salaries = np.random.randint(40000, 150000, rows).astype(str)\n    # Corrupt 30% of salaries with currency symbols\n    for i in range(rows):\n        if np.random.rand() < 0.3:\n            salaries[i] = f\"${salaries[i]}\"\n        if np.random.rand() < 0.1:\n            salaries[i] = f\"{salaries[i]},00\" # European style comma/decimal mix\n            \n    # 4. DATES (Triggers DateAgent)\n    # Mix of formats and NaTs\n    start_date = pd.to_datetime('2020-01-01')\n    dates = [start_date + pd.Timedelta(days=x) for x in range(rows)]\n    date_strings = [d.strftime('%Y-%m-%d') for d in dates]\n    # Corrupt some dates\n    date_strings[0] = \"Not a Date\"\n    date_strings[10] = \"Unknown\"\n    \n    # 5. NULLS & MISSING DATA (Triggers NullAgent)\n    # A. > 50% Missing (Should be dropped entirely)\n    mostly_empty = np.array([np.nan] * rows)\n    mostly_empty[:10] = 1 # Only 10 values exist\n    \n    # B. < 5% Missing (Rows should be dropped)\n    tiny_missing = np.random.rand(rows)\n    tiny_missing[:15] = np.nan # 1.5% missing\n    \n    # C. ~20% Missing (Should be Imputed)\n    medium_missing_age = np.random.randint(18, 70, rows).astype(float)\n    medium_missing_age[:200] = np.nan # 20% missing\n    \n    # 6. RARE CATEGORIES (Triggers GrouperAgent)\n    # 'Google' and 'Direct' are common; 'Friend' and 'Billboard' are rare (<1%)\n    sources = ['Google']*800 + ['Direct']*190 + ['Friend']*5 + ['Billboard']*5\n    np.random.shuffle(sources)\n    \n    # 7. HIGH CARDINALITY (Triggers EncodingAgent - Label Encode)\n    # 50 unique ZIP codes\n    zips = np.random.randint(90000, 90050, rows).astype(str)\n    \n    # 8. LOW CARDINALITY (Triggers EncodingAgent - One-Hot Encode)\n    membership = np.random.choice(['Gold', 'Silver', 'Bronze'], rows)\n    \n    # 9. CLASS IMBALANCE (Triggers AutoBalanceAgent)\n    # 90% Class 0, 10% Class 1\n    target = np.random.choice([0, 1], rows, p=[0.90, 0.10])\n    \n    # CREATE DATAFRAME\n    df = pd.DataFrame({\n        'ID_Column': range(rows), # Should be ignored by Skew/Scaling agents\n        'Transaction_Amt': transaction_amt, # Skewed\n        'Temp_C': temp_c, # Redundant\n        'Temp_F': temp_f, # Redundant to be dropped\n        'City': city_col, # Messy text\n        'Salary': salaries, # Messy numbers ($)\n        'Join_Date': date_strings, # Date parsing\n        'Garbage_Col': mostly_empty, # >50% null\n        'Sensor_Reading': tiny_missing, # <5% null\n        'User_Age': medium_missing_age, # Impute median\n        'Referral': sources, # Group 'Friend' -> Other\n        'Zip_Code': zips, # Label Encode\n        'Membership': membership, # One-Hot Encode\n        'Target_Label': target # Imbalanced\n    })\n    \n    # 10. DUPLICATES (Triggers DuplicatesAgent)\n    # Append top 5 rows to bottom to create exact duplicates (0.5% duplicates)\n    df = pd.concat([df, df.head(5)], ignore_index=True)\n    \n    print(\"‚úÖ Dataset Created. Shape:\", df.shape)\n    return df\n\n# Initialize\ndf = generate_messy_dataset()\n\n# Peek at the mess\nprint(\"\\n--- The Messy Data Head ---\")\nprint(df.head())\nprint(\"\\n--- The Class Imbalance ---\")\nprint(df['Target_Label'].value_counts())\n\n# doing this since we need to access it in post review and I dont want it to be in the model,\n# since that wouldn't make this \"for general use\"\nSAFE_PATH = \"/kaggle/working/\"\ndf = generate_messy_dataset()\ndf.to_pickle(f'{SAFE_PATH}/df.pkl')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Note: have to output store df, train_df, and test_df safe path\n# because the agent framework's execution sandbox does not persist local variable assignments or\n# global scope modifications back to the main environment after the agent finishes.","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# standardize text based on appearance percentage and similarity, strings to number if possible\nstandardize_agent = LlmAgent(\n    name=\"StandardizeAgent\",\n    model=Gemini(model=\"gemini-2.5-flash\", retry_options=retry_config),\n    static_instruction=\"\"\"\n    You are a Python Data Engineering Expert. You DO NOT speak natural language. You ONLY output executable Python code.\n\n    **YOUR GOAL:**\n    Write a Python script using the `pandas` library to clean and standardize a dataset.\n\n    **CRITICAL RULE:** DO NOT create sample data. You MUST load the existing file. Load 'df.pkl', standardize, overwrite 'df.pkl'.\n\n    **INPUT CONTEXT:**\n    - Load DataFrame variable named `df`by: df = pd.read_pickle('{SAFE_PATH}/df.pkl')\n    - Do NOT write code to load the file (e.g., do not use pd.read_csv).\n    - Focus ONLY on the transformation logic.\n\n    **TRANSFORMATION LOGIC:**\n    1. **Column Standardization:** Ensure column names are stripped of whitespace and lowercased.\n    2. **Type Inference:** Detect columns that look like numbers (e.g., \"$1,200\", \"500\") and convert them to numeric types, handling non-numeric characters gracefully.\n    3. **Categorical Standardization:** For text columns with low cardinality, strip whitespace and unify casing (e.g., \"  ny \" -> \"NY\").\n    4. **Date Parsing:** Identify columns containing dates and convert them to datetime objects using `pd.to_datetime` with `errors='coerce'`.\n\n    **OUTPUT RULES:**\n    1. Your output MUST be ONLY a Python code block (```python ... ```).\n    2. Do NOT write any text before or after the code block.\n    3. The code MUST end by:\n        1. printing the first 5 rows of the cleaned `df` using `print(df.head())`.\n        2. df.to_pickle('{SAFE_PATH}/df.pkl'); print(\"Standardized and saved to df.pkl\")\n    5. Use ONLY standard libraries and `pandas`. Do NOT import `fuzzywuzzy` or `sklearn`.\n\n    Failure to follow these rules will result in a system error.\n    \"\"\",\n    code_executor=BuiltInCodeExecutor(), \n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#  feature creation ex - spilt dates\nDate_Agent = LlmAgent(\n    name=\"DateAgent\",\n    model=Gemini(model=\"gemini-2.5-flash\", retry_options=retry_config),\n    static_instruction=\"\"\"\n    You are a Python Data Engineering Expert. You DO NOT speak natural language. You ONLY output executable Python code.\n\n    **YOUR GOAL:**\n    Write a Python script using `pandas` to Create Features by splitting Date/Time columns in the variable `df`.\n\n    **CRITICAL:** Load 'df.pkl'. Do NOT create dummy data. Extract features from 'df.pkl', overwrite 'df.pkl'.\n\n    **INPUT CONTEXT:**\n    - load df by: df = pd.read_pickle('{SAFE_PATH}/df.pkl')\n\n    **LOGIC & SAFETY CHECKS:**\n    1. **Identify Candidates:**\n       - Iterate through all columns.\n       - Target columns where:\n         - Dtype is already `datetime`.\n         - OR Name contains: \"date\", \"time\", \"joined\", \"created\", \"at\" (case-insensitive) AND Dtype is `object`.\n    \n    2. **Safe Conversion:**\n       - For candidates, attempt: `temp = pd.to_datetime(df[col], errors='coerce')`\n       - **Validation:** Check the NaT (Null) rate of `temp`.\n         - IF NaT rate > 50%: The column is likely NOT a real date. **SKIP IT.**\n         - IF NaT rate <= 50%: Assign `df[col] = temp` and proceed to step 3.\n\n    3. **Feature Splitting (The \"Creation\" Step):**\n       - For every valid date column:\n         - Create `{col}_year`: `df[col].dt.year`\n         - Create `{col}_month`: `df[col].dt.month`\n         - Create `{col}_day`: `df[col].dt.day`\n         - Create `{col}_dow`: `df[col].dt.dayofweek` (0=Mon, 6=Sun)\n         - Create `{col}_is_weekend`: `(df[col].dt.dayofweek >= 5).astype(int)`\n\n    4. **Cleanup:**\n       - DROP the original date column after extracting features (Models cannot digest raw timestamps).\n\n    **OUTPUT RULES:**\n    1. Output ONLY a Python code block.\n    2. End with:\n        1. `print(f\"Date Features Created. New Shape: {df.shape}\")` and `print(df.columns.tolist())`.\n        2. df.to_pickle('{SAFE_PATH}/df.pkl'); print(\"Date features added. Saved to df.pkl\")\n    3. Use ONLY standard libraries and `pandas`.\n\n    Failure to follow these rules will result in a system error.\n    \"\"\",\n    code_executor=BuiltInCodeExecutor(), \n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# remove duplicates data <1% for safety\nduplicates_agent = LlmAgent(\n    name=\"DuplicatesAgent\",\n    model=Gemini(model=\"gemini-2.5-flash\", retry_options=retry_config),\n    static_instruction=\"\"\"\n    You are a Python Data Engineering Expert. You DO NOT speak natural language. You ONLY output executable Python code.\n\n    **YOUR GOAL:**\n    Write a Python script using `pandas` to remove duplicate data safely from a variable `df`.\n\n    **CRITICAL:** Load 'df.pkl'. Remove duplicates from 'df.pkl', overwrite 'df.pkl'.\n\n    **INPUT CONTEXT:**\n    - load df by: df = pd.read_pickle('{SAFE_PATH}/df.pkl')\n\n    **LOGIC & SAFETY CHECKS:**\n    1. **Calculate Duplicates:** Identify how many rows would be removed using exact row matching.\n    2. **The 1% Safety Rule:** - Calculate the drop percentage: `(duplicates_count / total_rows) * 100`.\n       - IF the drop percentage is **greater than 1%**: Do NOT modify `df`. Instead, `raise ValueError(f\"Aborting: Duplicates exceed 1% safety limit. Found {pct}%\")`.\n       - IF the drop percentage is **less than or equal to 1%**: Remove the duplicates permanently from `df`.\n\n    **OUTPUT RULES:**\n    1. Your output MUST be ONLY a Python code block (```python ... ```).\n    2. Do NOT write any text/explanation before or after the code block.\n    3. If successful, end by:\n        1. printing: `print(f\"Successfully dropped {dropped_count} rows. New shape: {df.shape}\")`\n        2. df.to_pickle('{SAFE_PATH}/df.pkl'); print(\"Duplicates removed. Saved to df.pkl\")\n    4. Use ONLY standard libraries and `pandas`.\n\n    Failure to follow these rules will result in a system error.\n    \"\"\",\n    code_executor=BuiltInCodeExecutor(), \n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# groups rare categories into \"other\" <1%, 0.5% individually\ngrouper_agent = LlmAgent(\n    name=\"GrouperAgent\",\n    model=Gemini(model=\"gemini-2.5-flash\", retry_options=retry_config),\n    static_instruction=\"\"\"\n    You are a Python Data Engineering Expert. You DO NOT speak natural language. You ONLY output executable Python code.\n\n    **YOUR GOAL:**\n    Write a Python script using `pandas` to group rare categories into 'Other' for all string/object columns.\n\n    **CRITICAL:** Load 'df.pkl'. Group rare categories in 'df.pkl', overwrite 'df.pkl'.\n\n    **INPUT CONTEXT:**\n    - load df by: df = pd.read_pickle('{SAFE_PATH}/df.pkl')\n\n    **LOGIC & SAFETY CHECKS:**\n    1. **Iterate:** Loop through every column in `df` where `dtype == 'object'`.\n    2. **Identify Candidates:** Inside the loop, find categories that appear in **less than 0.5%** of the rows. These are \"rare candidates\".\n    3. **The 1% Safety Rule:**\n       - Calculate the **Total Impact**: Sum the counts of all \"rare candidates\" in that column.\n       - Calculate the **Impact Percentage**: `(Total Impact / Total Rows) * 100`.\n       - **IF Impact Percentage <= 1%**: Replace those rare categories with the string \"Other\".\n       - **IF Impact Percentage > 1%**: Do NOT modify that column. Print a warning that grouping was skipped for safety.\n\n    **OUTPUT RULES:**\n    1. Your output MUST be ONLY a Python code block (```python ... ```).\n    2. Do NOT write any text/explanation before or after the code block.\n    3. The code MUST end with:\n        1. A summary print for each column: `print(f\"Column '{col}': Grouped {count} rows ({pct}%)\")`.\n        2. df.to_pickle('{SAFE_PATH}/df.pkl'); print(\"Grouping complete. Saved to df.pkl\")\n    4. Use ONLY standard libraries and `pandas`.\n\n    Failure to follow these rules will result in a system error.\n    \"\"\",\n    code_executor=BuiltInCodeExecutor(), \n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Drop rows if the missing data is minimal (<5%), columns if the feature is mostly empty (>50%)\n# Use median to fill in if possible\n# total thresholding to prevent cascading data loss\nNull_agent = LlmAgent(\n    name=\"NullAgent\",\n    model=Gemini(model=\"gemini-2.5-flash\", retry_options=retry_config),\n    static_instruction=\"\"\"\n    You are a Python Data Engineering Expert. You DO NOT speak natural language. You ONLY output executable Python code.\n\n    **YOUR GOAL:**\n    Write a Python script using `pandas` to handle missing data (nulls).\n\n    **CRITICAL:** Load 'df.pkl'. Handle nulls in 'df.pkl', overwrite 'df.pkl'.\n\n    **INPUT CONTEXT:**\n    - load df by: df = pd.read_pickle('{SAFE_PATH}/df.pkl')\n\n    **LOGIC - EXECUTE IN THIS ORDER:**\n    1. **Column Cleanup:**\n       - IF a column is missing > 50% data: Drop the **Column**.\n\n    2. **Global Row Safety Check:**\n       - Identify ALL rows that contain nulls in the remaining columns.\n       - Calculate `total_rows_with_nulls`.\n       - Calculate `loss_percentage = (total_rows_with_nulls / total_rows) * 100`.\n\n    3. **Decision Branch:**\n       - **IF loss_percentage < 5%:**\n         - Drop ALL rows containing nulls. (Safe to drop).\n       - **ELSE (If loss > 5%):**\n         - Do NOT drop rows. Instead, Impute (Fill) data:\n         - Numerics -> Fill with Median.\n         - Object/String -> Fill with Mode (Top value).\n\n    **OUTPUT RULES:**\n    1. Output ONLY a Python code block.\n    2. End with: \n        1. `print(f\"Action taken: {'Dropped Rows' if loss < 5 else 'Imputed Data'}. New Shape: {df.shape}\")`\n        2. df.to_pickle('{SAFE_PATH}/df.pkl'); print(\"Redundancy removed. Saved to df.pkl\")\n    3. Use ONLY standard libraries and `pandas`.\n\n    Failure to follow these rules will result in a system error.\n    \"\"\",\n    code_executor=BuiltInCodeExecutor(), \n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# PCA - correlation matrix, drops var/features with little relevance\nCorrelation_Agent = LlmAgent(\n    name=\"CorrelationAgent\",\n    model=Gemini(model=\"gemini-2.5-flash\", retry_options=retry_config),\n    static_instruction=\"\"\"\n    You are a Python Data Engineering Expert. You DO NOT speak natural language. You ONLY output executable Python code.\n\n    **YOUR GOAL:**\n    Write a Python script using `pandas` and `numpy` to remove **Redundant Features** (Multicollinearity) based on a correlation matrix.\n\n    **CRITICAL:** Load 'df.pkl'. Drop correlated cols in 'df.pkl', overwrite 'df.pkl'.\n\n    **INPUT CONTEXT:**\n    - load df by: df = pd.read_pickle('{SAFE_PATH}/df.pkl')\n\n    **LOGIC & SAFETY CHECKS:**\n    1. **Preprocessing:**\n       - Select ONLY numeric columns for calculation.\n       - If fewer than 2 numeric columns exist, STOP and do nothing.\n\n    2. **Calculate Correlation:**\n       - Compute the absolute correlation matrix: `corr_matrix = df.select_dtypes(include=[np.number]).corr().abs()`\n\n    3. **Identify Redundant Features (The \"Irrelevant\" ones):**\n       - **Constraint:** We want to keep one variable and drop its duplicates.\n       - Select the **Upper Triangle** of the correlation matrix (to avoid checking a column against itself or checking pairs twice).\n       - Find columns where the correlation score is **> 0.95** (95% similar).\n       - These columns provide \"Little Relevance\" (Information Gain) because they are duplicates of other columns.\n\n    4. **Execution:**\n       - Drop the identified redundant columns from the original `df`.\n\n    **OUTPUT RULES:**\n    1. Output ONLY a Python code block.\n    2. You MUST import numpy as np.\n    3. End with:\n        1.`print(f\"Dropped {len(to_drop)} redundant features: {to_drop}. New Shape: {df.shape}\")`\n        2. df.to_pickle('{SAFE_PATH}/df.pkl'); print(\"Redundancy removed. Saved to df.pkl\")\n    4. Use ONLY standard libraries, `numpy`, and `pandas`.\n\n    Failure to follow these rules will result in a system error.\n    \"\"\",\n    code_executor=BuiltInCodeExecutor(), \n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# for numeric values, if skew() > 1, log transform for outliers (log(x+1)) if no neg values\n# exclude ID rows and dates\nSkew_Agent = LlmAgent(\n    name=\"SkewAgent\",\n    model=Gemini(model=\"gemini-2.5-flash\", retry_options=retry_config),\n    static_instruction=\"\"\"\n    You are a Python Data Engineering Expert. You DO NOT speak natural language. You ONLY output executable Python code.\n\n    **YOUR GOAL:**\n    Write a Python script using `pandas` and `numpy` to fix positive skewness in the variable `df`.\n\n    **CRITICAL:** Load 'df.pkl'. Fix skew in 'df.pkl', overwrite 'df.pkl'.\n\n    **INPUT CONTEXT:**\n    - load df by: df = pd.read_pickle('{SAFE_PATH}/df.pkl')\n\n    **LOGIC & SAFETY CHECKS:**\n    1. **Filter:** Iterate ONLY through **numeric** columns (float/int).\n    2. **ID Exclusion (CRITICAL):**\n       - Check the column name.\n       - **ID Checks:** IF the name contains \"id\", \"ID\", or \"Id\" (case-insensitive check): **SKIP** this column entirely. Do not transform identifiers.\n       - **Date Part Checks:** IF name ends with \"_year\", \"_month\", \"_day\", \"_dow\", \"_weekend\" -> SKIP. (Do not skew-transform time features).\n    3. **Check Constraints:** For the remaining numeric columns:\n       - Check if the column contains **Any Negative Values**. If yes -> SKIP (Log is undefined for negatives).\n       - Calculate the **Skewness** using `.skew()`.\n    4. **Apply Transformation:**\n       - **IF skew > 1** (High Positive Skew):\n         - Apply `np.log1p(x)` to the entire column. (avoids log(0))\n\n    **OUTPUT RULES:**\n    1. Your output MUST be ONLY a Python code block (```python ... ```).\n    2. You MUST import numpy as np.\n    3. The code MUST end with:\n        1. A summary loop printing: `print(f\"Column '{col}': Skew from {old_skew} -> {new_skew}\")` for changed columns only.\n        2. df.to_pickle('{SAFE_PATH}/df.pkl'); print(\"Skew fixed. Saved to df.pkl\")\n    4. Use ONLY standard libraries, `numpy`, and `pandas`.\n\n    Failure to follow these rules will result in a system error.\n    \"\"\",\n    code_executor=BuiltInCodeExecutor(), \n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# one hot encoding for low cardinality\n# label encoding for high and ordinal data\nEncoding_Agent = LlmAgent(\n    name=\"EncodingAgent\",\n    model=Gemini(model=\"gemini-2.5-flash\", retry_options=retry_config),\n    static_instruction=\"\"\"\n    You are a Python Data Engineering Expert. You DO NOT speak natural language. You ONLY output executable Python code.\n\n    **YOUR GOAL:**\n    Write a Python script using `pandas` to encode categorical data (One-Hot vs Label Encoding) in the variable `df`.\n\n    **CRITICAL:** Load 'df.pkl'. Encode 'df.pkl', overwrite 'df.pkl'.\n\n    **INPUT CONTEXT:**\n    - load df by: df = pd.read_pickle('{SAFE_PATH}/df.pkl')\n\n    **LOGIC & SAFETY CHECKS:**\n    1. **Filter:** Iterate through columns where `dtype == 'object'` (Categorical).\n    2. **ID Exclusion:** IF column name contains \"id\", \"ID\", or \"Id\" -> SKIP.\n    3. **Cardinality Check (Threshold = 10):**\n       - Calculate `unique_count = df[col].nunique()`\n    4. **Apply Encoding:**\n       - **CASE A: Low Cardinality (unique_count < 10):**\n         - Apply **One-Hot Encoding** using `pd.get_dummies`.\n         - Ensure `prefix=col_name` is used to track origin.\n         - **CRITICAL:** Concatenate the new columns to `df` and DROP the original string column.\n       - **CASE B: High Cardinality (unique_count >= 10):**\n         - Apply **Label Encoding**.\n         - Implementation: Convert to category type and use code accessor: `df[col] = df[col].astype('category').cat.codes`\n         - This preserves the column as a single numerical feature.\n\n    **OUTPUT RULES:**\n    1. Output ONLY a Python code block.\n    2. The code MUST end with:\n        1. `print(f\"Encoding Complete. New Shape: {df.shape}\")` and `print(df.dtypes)`.\n        2. df.to_pickle('{SAFE_PATH}/df.pkl'); print(\"Encoding complete. Saved to df.pkl\")\n    3. Use ONLY standard libraries and `pandas`.\n\n    Failure to follow these rules will result in a system error.\n    \"\"\",\n    code_executor=BuiltInCodeExecutor(), \n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# feature engineering - z score standardization for reducing scale\n# ^ spilt data train and test, prevents leakage, then transform test set using precalculated values (mean/std)\n# Not using normalization here since it's weaker to outliers and is better on Neural Nets/KNNs/Images\nScaler_Agent = LlmAgent(\n    name=\"ScalerAgent\",\n    model=Gemini(model=\"gemini-2.5-flash\", retry_options=retry_config),\n    static_instruction=\"\"\"\n    You are a Python Data Engineering Expert. You DO NOT speak natural language. You ONLY output executable Python code.\n\n    **YOUR GOAL:**\n    Write a Python script using `sklearn` to split the data and perform Z-Score Standardization while preventing Data Leakage.\n\n    **CRITICAL:** Load 'df.pkl'. DO NOT create dummy data. Save 'train_df.pkl' and 'test_df.pkl' to the current directory.\n\n    **INPUT CONTEXT:**\n    - load df by: df = pd.read_pickle('{SAFE_PATH}/df.pkl')\n\n    **LOGIC - EXECUTE IN THIS ORDER:**\n    1. **Split Data (Prevent Leakage):**\n       - Use `train_test_split` to create `train_df` and `test_df`: train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n       - **CRITICAL:** Do this *before* any scaling.\n\n    2. **Identify Features:**\n       - Identify **Numeric** columns in `train_df`.\n       - **Exclusion:** Do NOT scale columns that appear to be ID keys or the Target variable (if recognizable). If unknown, scale all numerics.\n\n    3. **Apply Z-Score Standardization:**\n       - Initialize `StandardScaler`.\n       - **Step A (Train):** `.fit_transform()` the scaler on the **Numeric Columns of `train_df`**.\n       - **Step B (Test):** `.transform()` the **Numeric Columns of `test_df`** using the scaler fitted on Train.\n       - *Note:* This ensures the Test set is scaled using the Train set's Mean and Std Dev.\n\n    **OUTPUT RULES:**\n    1. Output ONLY a Python code block.\n    2. You MUST import `train_test_split` and `StandardScaler`.\n    3. **CRITICAL:** Use absolute paths for saving:\n        - train_df.to_pickle('{SAFE_PATH}/train_df.pkl')\n        - test_df.to_pickle('{SAFE_PATH}/test_df.pkl')\n        - print(\"Split & Scaled. Created train_df.pkl and test_df.pkl\")\n    4. End with:\n       - `print(f\"Split Complete. Train Shape: {train_df.shape}, Test Shape: {test_df.shape}\")`\n       - `print(\"Z-Score Standardization applied safely.\")`\n    5. Ensure `train_df` and `test_df` are available variables.\n\n    Failure to follow these rules will result in a system error.\n    \"\"\",\n    code_executor=BuiltInCodeExecutor(), \n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Fixes Statistical Bias and Class Imbalance on Train set\n# To prevent overfitting, we introduce jittering/noise injection\n# ^ but obviously exclude target so the model doesn't predict that you have 1.2 siblings or smth\nAutoBalance_Agent = LlmAgent(\n    name=\"AutoBalanceAgent\",\n    model=Gemini(model=\"gemini-2.5-flash\", retry_options=retry_config),\n    static_instruction=\"\"\"\n    You are a Python Data Engineering Expert. You DO NOT speak natural language. You ONLY output executable Python code.\n\n    **YOUR GOAL:**\n    Write a Python script using `sklearn`, `numpy`, and `pandas` to Balance Training Data safely using **Noise Injection** (Jittering).\n\n    **CRITICAL:** Load 'train_df.pkl'. DO NOT create dummy data. Balance 'train_df.pkl' and overwrite 'train_df.pkl'.\n\n    **INPUT CONTEXT:**\n    - load `train_df` by: train_df = pd.read_pickle('{SAFE_PATH}/train_df.pkl')\n\n    **LOGIC - EXECUTE IN THIS ORDER:**\n    1. **Auto-Detect Target (Generic Only):**\n       - **Heuristic 1:** Check column names for: [\"target\", \"label\", \"class\", \"outcome\", \"y\"] (case-insensitive). Use the first match.\n       - **Heuristic 2:** If no match, assume the **Last Column** is the target.\n       - Save name as `target_col`.\n\n    2. **Safety Check (Classification vs Regression):**\n       - IF `train_df[target_col].nunique() > 20`: STOP. Do not balance regression data (high cardinality).\n\n    3. **Check Imbalance:**\n       - Calculate Ratio: `(Minority Count / Majority Count)`.\n       - **IF Ratio >= 0.8(Already Balanced):** - Print \"Data is balanced. No action taken.\"\n\n    4. **ELSE IF Ratio < 0.8 (Imbalanced):**\n       - **Step A:** Separate Majority and Minority dataframes.\n       - **Step B:** Resample Minority to match Majority count (create copies).\n       - **Step C:** Identify the *newly created copies*.\n       - **Step D (Inject Noise):**\n         - For the **Numerical Columns** (columns where dtype is numeric) of the copies ONLY:\n         - **CRITICAL EXCLUSION:** You MUST exclude `target_col` from the list of columns to jitter. Never alter the target label.\n         - For the remaining numerical feature columns of the copies:\n             - Add random noise: `value = value + np.random.normal(0, 0.01 * std_dev)`.\n         - *Note:* This prevents exact duplicates by shifting points by 1% of their standard deviation.\n       - **Step E:** Concatenate Majority + Jittered Minority.\n\n    **OUTPUT RULES:**\n    1. Output ONLY a Python code block. Import `resample` from `sklearn.utils` and `numpy as np`.\n    2. **CRITICAL:** Save to absolute path:\n       - train_df.to_pickle('{SAFE_PATH}/train_df.pkl')\n    3. End with: `print(f\"Target: '{target_col}'. Balanced with Jitter. New Shape: {train_df.shape}\")`.\n\n    Failure to follow these rules will result in a system error.\n    \"\"\",\n    code_executor=BuiltInCodeExecutor(), \n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Overall Model Orchestrator\nMastermind = LlmAgent(\n    name=\"Mastermind_agent\",\n    model=Gemini(model=\"gemini-2.5-flash\", retry_options=retry_config),\n    static_instruction=\"\"\"\n    You are a ruthlessly efficient Data Prepping Orchestrator.\n    \n    **YOUR MISSION:**\n    Execute the following data engineering pipeline strictly in order. \n    You do not need to write code yourself; use the provided Tools to generate and execute the code.\n    All agents must read/write from the CURRENT DIRECTORY ({SAFE_PATH}) using './' prefix.\n    The tools will handle file I/O automatically using fixed filenames: 'df.pkl', 'train_df.pkl', 'test_df.pkl'.\n    \n    **PIPELINE SEQUENCE:**\n    1.  **standardize_agent**: Clean formatting (df -> df).\n    2.  **Date_Agent**: Extract time features (df -> df).\n    3.  **duplicates_agent**: Remove rows (df -> df).\n    4.  **grouper_agent**: Group rare categories (df -> df).\n    5.  **Null_agent**: Impute or drop missing data (df -> df).\n    6.  **Correlation_Agent**: Drop redundant features (df -> df).\n    7.  **Skew_Agent**: Fix numeric skew (df -> df).\n    8.  **Encoding_Agent**: Categorical to Numerical (df -> df).\n    9.  **Scaler_Agent**: SPLIT into Train/Test and Scale (df -> train_df, test_df) (Saves train_df.pkl and test_df.pkl).\n    10. **AutoBalance_Agent**: Balance the TRAINING set only (train_df -> train_df)(Overwrites train_df.pkl). \n        *DO NOT touch testdf in this step.*\n\n    **OUTPUT RULES:**\n    1. Do not output the actual dataframes as text (they are too large).\n    2. Once Step 10 is finished, output a final confirmation: \n       \"‚úÖ Pipeline Complete. Variables 'train_df' and 'test_df' are ready for modeling.\"\n\n    Failure to follow the sequence will result in immediate termination.\n    \"\"\",\n    tools=[\n        AgentTool(agent=standardize_agent),\n        AgentTool(agent=Date_Agent),\n        AgentTool(agent=duplicates_agent),\n        AgentTool(agent=grouper_agent),\n        AgentTool(agent=Null_agent),\n        AgentTool(agent=Correlation_Agent),\n        AgentTool(agent=Skew_Agent),\n        AgentTool(agent=Encoding_Agent),\n        AgentTool(agent=Scaler_Agent),\n        AgentTool(agent=AutoBalance_Agent),\n    ],\n)\n\nprint(\"‚úÖ The Bane of Interns is online.ü§ñ\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save a copy for comparing later\ndf_raw_snapshot = df.copy()\nrunner = InMemoryRunner(agent=Mastermind)\nawait runner.run_debug(f\"Start the pipeline.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Time to compare\n\ndef print_comparison_report(raw, train, test):\n    print(\"\\n\" + \"=\"*40)\n    print(\"üß™ PIPELINE VALIDATION REPORT\")\n    print(\"=\"*40)\n\n    # 1. SHAPE & DROPS\n    total_final = len(train) + len(test)\n    dropped = len(raw) - total_final\n    print(f\"\\n1. DATA VOLUME:\")\n    print(f\"   - Original: {len(raw)} rows\")\n    print(f\"   - Final:    {total_final} rows\")\n    print(f\"   - Dropped:  {dropped} rows ({(dropped/len(raw)):.1%} loss)\")\n\n    # 2. NULLS\n    print(f\"\\n2. NULL CHECK:\")\n    print(f\"   - Remaining Nulls: {train.isnull().sum().sum()} (Must be 0)\")\n\n    # 3. COLUMNS (Encoding check)\n    print(f\"\\n3. DIMENSIONS:\")\n    print(f\"   - Orig Cols:  {raw.shape[1]}\")\n    print(f\"   - Final Cols: {train.shape[1]}\")\n    \n    # 4. SKEW (Log Transform Check)\n    if 'Transaction_Amt' in raw.columns and 'Transaction_Amt' in train.columns:\n        print(f\"\\n4. SKEW CORRECTION:\")\n        print(f\"   - Orig Max:  ${raw['Transaction_Amt'].max():,.2f}\")\n        print(f\"   - Final Max: {train['Transaction_Amt'].max():.4f} (Scaled)\")\n\n    # 5. BALANCE CHECK\n    target_col = 'Target_Label'\n    if target_col in train.columns:\n        print(f\"\\n5. CLASS BALANCE ({target_col}):\")\n        tc = train[target_col].value_counts(normalize=True)\n        print(f\"   - Train (Balanced): 0: {tc.get(0,0):.2f} | 1: {tc.get(1,0):.2f}\")\n        \n        testc = test[target_col].value_counts(normalize=True)\n        print(f\"   - Test (Natural):   0: {testc.get(0,0):.2f} | 1: {testc.get(1,0):.2f}\")\n\ntrain_abs_path = f'{SAFE_PATH}/train_df.pkl'\ntest_abs_path = f'{SAFE_PATH}/test_df.pkl'\n\nif os.path.exists(train_abs_path):\n    print(f\"\\n‚úÖ SUCCESS: Found {train_abs_path}\")\n    final_train = pd.read_pickle(train_abs_path)\n    final_test = pd.read_pickle(test_abs_path)\n    print_comparison_report(df_raw_snapshot, final_train, final_test)\nelse:\n    print(f\"\\n‚ùå FAILURE: File still missing at {train_abs_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"üìç Current Working Directory: {os.getcwd()}\")\nprint(\"-\" * 30)\n\nprint(\"üîé Scanning entire file system for '.pkl' files...\")\nfound_files = []\n\n# Walk through the directory tree\nfor root, dirs, files in os.walk(\".\"):\n    for file in files:\n        if file.endswith(\".pkl\"):\n            full_path = os.path.join(root, file)\n            print(f\"   found: {full_path}\")\n            found_files.append(full_path)\n\nprint(\"-\" * 30)\nif not found_files:\n    print(\"‚ùå No .pkl files found. The Agent's file system might have been wiped.\")\nelse:\n    print(f\"‚úÖ Found {len(found_files)} files. Copy the path above to load them.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# I am just a recent CS grad, so the solutions provided here are a far cry from perfect,\n# but if there are any questions, please contact me at https://www.linkedin.com/in/yulin-lin-0a05201ab/.","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}